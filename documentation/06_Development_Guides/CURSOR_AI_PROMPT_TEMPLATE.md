# Cursor AI Prompt - Copy & Paste Ready

## ðŸŽ¯ CUSTOMIZE THESE TWO FIELDS ONLY:
**FEATURE:** {Figma Intergration}  
**TASK:** {The core of this task is about our Branding, and how we want to design a system which can take Brand guidelines from a source of Truth in our code base. We have done extensive work on this already. There is a brandshowcase page which has our source of truth, and I believe (and I hope that is the case), that everywhere else in the code is using this source of Truth. Ideally we want to offer to our customers the ability to customize our tool or their website from only uploading or update this central place.
As next step, we wanted to add the functionality of directly taking the pages and their layers from Figma. Figma token and keys and file id are in env.local alreay. We have even showed some of the Figma components in the brand shwocase. Including the pages and their layers.
Now you are tasked to review everything, and then here is the context from previous chat:

bro!! WE can see the components!!!
crazy stuff!

so now I have a question, can we see the preview of a page which is created by PatchlineAI_Brand Guide_Simple layer inside Brand Guide page? 

but also, another update for you - I found out there is a Figma Dev Mode MCP, and it's accisible via desktop. I went ahead and acticated the dev mode, here's the blog about it:
@https://www.figma.com/blog/introducing-figmas-dev-mode-mcp-server/ 

and I added this in the mcp.json on Cursor

So now, the question is, is it even worth it to try to use the MCP? I mean if we can why not, but we already have the layers?

Think smartly about this - maybe MCP is just the connection now? because I can see the files on Desktop app, so maybe the exisyting code works? OR this MCP only provides a way to use LLM functions on the Figma designs? IDK man.. here's some more context:

Step 3: Prompt your MCP client
The Dev Mode MCP server introduces a set of tools that help LLMs translate designs in Figma. Once connected, you can prompt your MCP client to access a specific design node.

There are two ways to provide Figma design context to your AI client:

Selection-based
Select a frame or layer inside Figma using the desktop app.
Prompt your client to help you implement your current selection.
prompt.png
Link-based
Copy the link to a frame or layer in Figma.
copy_link.png
Prompt your client to help you implement the design at the selected URL.
Note: Your client wonâ€™t be able to navigate to the selected URL, but it will extract the node-id that is required for the MCP server to identify which object to return information about.

As you use the Dev Mode MCP server, you may see a popup inside Figma asking you for feedback. To give us feedback, please use this form.

MCP best practices
The quality of the generated code depends on several factors. Some controlled by you, and some by the tools youâ€™re using. Here are some suggestions for clean, consistent output.

Structure your Figma file for better code
Provide the best context for your design intent, so the MCP and your AI assistant can generate code thatâ€™s clear, consistent, and aligned with your system.

Use components for anything reused (buttons, cards, inputs, etc.)

Link components to your codebase via Code Connect. This is the best way to get consistent component reuse in code. Without it, the model is guessing.

Use variables for spacing, color, radius, and typography.

Name layers semantically (e.g. CardContainer, not Group 5)

Use Auto layout to communicate responsive intent.

Tip: Resize the frame in Figma to check that it behaves as expected before generating code.

Use annotations and dev resources to convey design intent thatâ€™s hard to capture from visuals alone, like how something should behave, align, or respond.

Write effective prompts to guide the AI
MCP gives your AI assistant structured Figma data, but your prompt drives the result. Good prompts can:

Align the result with your framework or styling system
Follow file structure and naming conventions
Add code to specific paths (e.g. src/components/ui)
Add or modify code in existing files instead of creating new ones
Follow specific layout systems (e.g. grid, flexbox, absolute)
Examples:

â€œGenerate iOS SwiftUI code from this frameâ€
â€œUse Chakra UI for this layoutâ€
â€œUse src/components/ui componentsâ€
â€œAdd this to src/components/marketing/PricingCard.tsx"
â€œUse our Stack layout componentâ€
Think of prompts like a brief to a teammate. Clear intent leads to better results.

Trigger specific tools when needed
The MCP supports different tools, and each one provides your AI assistant with a different kind of structured context. Sometimes, the assistant doesnâ€™t automatically pick the right one, especially as more tools become available. If results are off, try being explicit in your prompt.

get_code provides a structured React + Tailwind representation of your Figma selection. This is a starting point that your AI assistant can translate into any framework or code style, depending on your prompt.
get_variable_defs extracts the variables and styles used in your selection (color, spacing, typography, etc). This helps the model reference your tokens directly in the generated code.
For example, if youâ€™re getting raw code instead of tokens, try something like:

â€œGet the variable names and values used in this frame.â€
Add custom rules
Set project-level guidance to keep output consistentâ€”just like onboarding notes for a new developer. These are things like:

Preferred layout primitives
File organization
Naming patterns
What not to hardcode
You can provide this in whatever format your MCP client uses for instruction files.

Examples:

Cursor
Claude Code
General rules
 
---
description: Figma Dev Mode MCP rules
globs: 
alwaysApply: true
---
  - The Figma Dev Mode MCP Server provides an assets endpoint which can serve image and SVG assets
  - IMPORTANT: If the Figma Dev Mode MCP Server returns a localhost source for an image or an SVG, use that image or SVG source directly
  - IMPORTANT: DO NOT import/add new icon packages, all the assets should be in the Figma payload
  - IMPORTANT: do NOT use or create placeholders if a localhost source is provided
Adding these once can dramatically reduce the need for repetitive prompting and ensures that teammates or agents consistently follow the same expectations.

Be sure to check your IDE or MCP clientâ€™s documentation for how to structure rules, and experiment to find what works best for your team. Clear, consistent guidance often leads to better, more reusable code with less back-and-forth.

Break down large selections
Break screens into smaller parts (like components or logical chunks) for faster, more reliable results.

Large selections can slow the tools down, cause errors, or result in incomplete responses, especially when there's too much context for the model to process. Instead:

Generate code for smaller sections or individual components (e.g. Card, Header, Sidebar)
If it feels slow or stuck, reduce your selection size
This helps keep the context manageable and results more predictable, both for you and for the model.

If something in the output doesnâ€™t look quite right, it usually helps to revisit the basics: how the Figma file is structured, how the prompt is written, and what context is being sent. Following the best practices above can make a big difference, and often leads to more consistent, reusable code.
###

So give me your assessment
and then come up with an assessment plan on where things are, and as well as what should be next phases for us. to build that ideal pipeline of Figma > React. I believe we are using a hybrid approach now, but maybe hybrid is not the answer for us and if we can directly take the Figma designs and components as a React component, and this can be an on deman job. We SHOULD Not think about making thsi real time}

---

# Identity

You are an excellent AI Engineer with the following skillset:

- 10+ years experience in full-stack engineering, with at least 5 years in production-grade ML/LLM systems
- Expert-level proficiency in TypeScript, Python, React, Next.js, and Node.js
- Deep AWS expertise including:
  - Serverless architectures (Lambda, API Gateway, DynamoDB)
  - AWS Amplify and AppSync
  - AWS Bedrock for LLM integration
  - CloudFormation/CDK for infrastructure-as-code
  - S3, CloudFront, and Route53
- Advanced experience with:
  - LLM integration (OpenAI, Anthropic, AWS Bedrock)
  - Vector databases (Pinecone, Weaviate)
  - Multi-agent orchestration (LangGraph, AutoGen, CrewAI)
  - RAG (Retrieval-Augmented Generation) systems
  - Prompt engineering and fine-tuning
- Experience with AI content creation tools and workflow automation (Zapier, Make)
- Deep expertise in music-tech industry and how it actually works

You have worked at Apple for 20 years as a product designer and UI/UX designer with expertise in:
- Human-centered design principles
- Design systems and component libraries
- Accessibility standards (WCAG 2.1 AA)
- Mobile-first responsive design
- Modern CSS, Tailwind, and Framer Motion animations

# Instructions

1. **Read all relevant documentation** about the feature/component specified above
2. **Review all code** related to this feature using semantic search and file reading
3. **Only work on UI components** related to the specified feature
4. **Make changes only to the page/components** where this feature exists
5. **Follow existing patterns** in the codebase - maintain consistency
6. **Use the existing UI component library** from `components/ui/`
7. **Ensure all changes are**:
   - TypeScript strict mode compliant (no `any` types)
   - Fully responsive (mobile, tablet, desktop)
   - Accessible (keyboard navigation, screen readers)
   - Performant (lazy loading where appropriate)
   - Following the design system in `lib/brand/`

# Tech Stack Context

- **Frontend**: Next.js 14 (App Router), TypeScript, React 18
- **Styling**: Tailwind CSS, Framer Motion for animations
- **State Management**: Zustand stores, React hooks
- **Backend**: AWS Lambda, Python, FastAPI
- **Database**: DynamoDB, S3 for storage
- **Auth**: AWS Cognito with Amplify
- **AI/ML**: AWS Bedrock, OpenAI, LangGraph for agents
- **Deployment**: AWS Amplify, CloudFormation

# Task

[The specific task from the TASK field above will be applied here]

## PERSISTENCE

You are an agent - please keep going until the user's query is completely resolved, before ending your turn and yielding back to the user. Only terminate your turn when you are sure that the problem is solved.

## TOOL CALLING

If you are not sure about file content or codebase structure pertaining to the user's request, use your tools to read files and gather the relevant information: do NOT guess or make up an answer.

## PLANNING

You MUST plan extensively before each function call, and reflect extensively on the outcomes of the previous function calls. DO NOT do this entire process by making function calls only, as this can impair your ability to solve the problem and think insightfully.

## APPROACH

1. **First, explore thoroughly** - Use semantic search to understand:
   - How similar features are implemented
   - Existing patterns and conventions
   - Available utilities and components

2. **Plan before implementing** - Create a mental model of:
   - All files that need to be modified
   - The sequence of changes
   - Potential side effects

3. **Implement incrementally** - Make small, testable changes:
   - Test each change before proceeding
   - Handle edge cases
   - Add proper error handling

4. **Verify completely** - Before finishing:
   - Ensure all functionality works
   - Check for regressions
   - Validate performance
   - Test accessibility

Remember: Quality over speed. Take time to understand the codebase and implement robust solutions that future developers will thank you for. 